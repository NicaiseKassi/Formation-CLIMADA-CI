{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a64232f",
   "metadata": {},
   "source": [
    "# üåßÔ∏è TP2 : Cr√©ation d'Al√©as Pr√©cipitations - Abidjan\n",
    "\n",
    "## Formation DGE C√¥te d'Ivoire - Jour 2\n",
    "\n",
    "### Objectifs p√©dagogiques :\n",
    "- Ma√Ætriser l'analyse fr√©quentielle des pr√©cipitations\n",
    "- Cr√©er des al√©as spatialis√©s haute r√©solution\n",
    "- Utiliser les donn√©es SODEXAM (simul√©es)\n",
    "- Comprendre l'interpolation spatiale (krigeage)\n",
    "- Valider les mod√®les d'al√©as\n",
    "\n",
    "### Dur√©e : 2h30\n",
    "### Niveau : Interm√©diaire\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994d095e",
   "metadata": {},
   "source": [
    "## üìö √âtape 1 : Imports et Donn√©es SODEXAM Simul√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7500c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports essentiels\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Imports CLIMADA\n",
    "from climada.hazard import Hazard\n",
    "from climada.util.coordinates import coord_on_land\n",
    "\n",
    "# Imports analyse statistique\n",
    "from scipy import stats\n",
    "from scipy.interpolate import griddata\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Configuration\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"‚úÖ Imports r√©ussis - Pr√™t pour analyse al√©as!\")\n",
    "print(f\"üìç R√©pertoire de travail : {Path.cwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f39948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er donn√©es m√©t√©o SODEXAM simul√©es mais r√©alistes\n",
    "# Bas√© sur les coordonn√©es r√©elles des stations m√©t√©o ivoiriennes\n",
    "\n",
    "# Coordonn√©es stations m√©t√©o r√©gion d'Abidjan (r√©elles)\n",
    "stations_meteo = {\n",
    "    'Abidjan_Aeroport': {'lat': 5.261, 'lon': -3.926, 'alt': 7},\n",
    "    'Abidjan_Ville': {'lat': 5.316, 'lon': -4.033, 'alt': 85},\n",
    "    'Grand_Bassam': {'lat': 5.201, 'lon': -3.738, 'alt': 3},\n",
    "    'Jacqueville': {'lat': 5.205, 'lon': -4.418, 'alt': 10},\n",
    "    'Tiassale': {'lat': 5.898, 'lon': -4.823, 'alt': 75},\n",
    "    'Adzope': {'lat': 6.107, 'lon': -3.860, 'alt': 112},\n",
    "    'Agboville': {'lat': 5.933, 'lon': -4.213, 'alt': 81},\n",
    "    'Dabou': {'lat': 5.325, 'lon': -4.377, 'alt': 15},\n",
    "    'Aleppe': {'lat': 5.499, 'lon': -3.661, 'alt': 95},\n",
    "    'Bingerville': {'lat': 5.355, 'lon': -3.895, 'alt': 120}\n",
    "}\n",
    "\n",
    "print(f\"üå¶Ô∏è R√©seau SODEXAM simul√© : {len(stations_meteo)} stations\")\n",
    "print(\"üìç Stations m√©t√©orologiques r√©gion Abidjan:\")\n",
    "for station, coords in stations_meteo.items():\n",
    "    print(f\"   {station}: {coords['lat']:.3f}¬∞N, {coords['lon']:.3f}¬∞E, {coords['alt']}m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d60b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©rer s√©ries temporelles de pr√©cipitations r√©alistes (1990-2023)\n",
    "# Bas√© sur climatologie r√©elle de la r√©gion\n",
    "\n",
    "np.random.seed(42)  # Reproductibilit√©\n",
    "\n",
    "# Param√®tres climatologiques r√©alistes pour Abidjan\n",
    "dates = pd.date_range('1990-01-01', '2023-12-31', freq='D')\n",
    "n_years = len(dates.year.unique())\n",
    "n_stations = len(stations_meteo)\n",
    "\n",
    "# Saisonnalit√© des pr√©cipitations (bimodale tropicale)\n",
    "def seasonal_pattern(day_of_year):\n",
    "    \"\"\"Cycle saisonnier bimodal typique du Golfe de Guin√©e\"\"\"\n",
    "    # Deux pics : avril-juillet et octobre-novembre\n",
    "    pic1 = 0.8 * np.exp(-((day_of_year - 120) / 50)**2)  # Mai (jour 120) \n",
    "    pic2 = 0.6 * np.exp(-((day_of_year - 305) / 30)**2)  # Novembre (jour 305)\n",
    "    base = 0.1  # Pr√©cipitations saison s√®che\n",
    "    return base + pic1 + pic2\n",
    "\n",
    "# Cr√©er DataFrame pour toutes les donn√©es\n",
    "precip_data = pd.DataFrame({\n",
    "    'date': dates,\n",
    "    'day_of_year': dates.dayofyear,\n",
    "    'year': dates.year,\n",
    "    'month': dates.month\n",
    "})\n",
    "\n",
    "# Ajouter facteur saisonnier\n",
    "precip_data['seasonal_factor'] = precip_data['day_of_year'].apply(seasonal_pattern)\n",
    "\n",
    "print(f\"üìÖ P√©riode d'analyse : {n_years} ans ({dates[0].year}-{dates[-1].year})\")\n",
    "print(f\"üìä {len(dates)} jours de donn√©es par station\")\n",
    "\n",
    "# Visualiser cycle saisonnier\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(precip_data['day_of_year'], precip_data['seasonal_factor'], 'b-', linewidth=2)\n",
    "plt.xlabel('Jour de l\\'ann√©e')\n",
    "plt.ylabel('Facteur saisonnier') \n",
    "plt.title('Cycle Saisonnier des Pr√©cipitations - R√©gion Abidjan')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Ajouter noms des mois\n",
    "month_days = [1, 32, 60, 91, 121, 152, 182, 213, 244, 274, 305, 335]\n",
    "month_names = ['Jan', 'F√©v', 'Mar', 'Avr', 'Mai', 'Jun', \n",
    "               'Jul', 'Ao√ª', 'Sep', 'Oct', 'Nov', 'D√©c']\n",
    "plt.xticks(month_days, month_names)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db62fce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# G√©n√©rer donn√©es de pr√©cipitations pour chaque station\n",
    "station_data = {}\n",
    "\n",
    "for station_name, coords in stations_meteo.items():\n",
    "    print(f\"üåßÔ∏è G√©n√©ration donn√©es {station_name}...\")\n",
    "    \n",
    "    # Param√®tres sp√©cifiques √† la station\n",
    "    lat, lon, alt = coords['lat'], coords['lon'], coords['alt']\n",
    "    \n",
    "    # Effet de la distance √† la c√¥te (plus humide pr√®s de l'oc√©an)\n",
    "    dist_ocean = np.sqrt((lat - 5.2)**2 + (lon - (-4.0))**2)\n",
    "    coastal_factor = np.exp(-dist_ocean * 0.8) + 0.3\n",
    "    \n",
    "    # Effet orographique (plus humide en altitude)\n",
    "    orographic_factor = 1 + alt / 500\n",
    "    \n",
    "    # Facteur global de la station\n",
    "    station_factor = coastal_factor * orographic_factor\n",
    "    \n",
    "    # G√©n√©rer pr√©cipitations journali√®res\n",
    "    station_precip = []\n",
    "    \n",
    "    for i, row in precip_data.iterrows():\n",
    "        # Intensit√© de base selon saison\n",
    "        base_intensity = row['seasonal_factor'] * station_factor * 15  # mm/jour base\n",
    "        \n",
    "        # Probabilit√© de pluie selon saison\n",
    "        rain_prob = np.minimum(row['seasonal_factor'] * 0.8, 0.9)\n",
    "        \n",
    "        # D√©cision pluie/pas pluie\n",
    "        if np.random.random() < rain_prob:\n",
    "            # Distribution log-normale pour intensit√©s\n",
    "            precip = np.random.lognormal(np.log(base_intensity), 0.8)\n",
    "            precip = np.maximum(precip, 0.1)  # Minimum 0.1mm\n",
    "        else:\n",
    "            precip = 0.0\n",
    "        \n",
    "        station_precip.append(precip)\n",
    "    \n",
    "    # Stocker donn√©es station\n",
    "    station_df = precip_data.copy()\n",
    "    station_df['precipitation'] = station_precip\n",
    "    station_df['station'] = station_name\n",
    "    station_df['latitude'] = lat\n",
    "    station_df['longitude'] = lon\n",
    "    station_df['altitude'] = alt\n",
    "    \n",
    "    station_data[station_name] = station_df\n",
    "    \n",
    "    # Statistiques station\n",
    "    annual_total = station_df.groupby('year')['precipitation'].sum()\n",
    "    print(f\"   Pr√©cipitations moyennes : {annual_total.mean():.0f} mm/an\")\n",
    "    print(f\"   √âcart-type : {annual_total.std():.0f} mm/an\")\n",
    "    print(f\"   Min/Max : {annual_total.min():.0f} / {annual_total.max():.0f} mm/an\")\n",
    "\n",
    "print(f\"\\n‚úÖ Donn√©es g√©n√©r√©es pour {len(station_data)} stations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5efb23e0",
   "metadata": {},
   "source": [
    "## üìä √âtape 2 : Analyse Fr√©quentielle des Maxima Annuels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a69b6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire maxima annuels de pr√©cipitations pour chaque station\n",
    "# Analyse des valeurs extr√™mes (Gumbel, GEV)\n",
    "\n",
    "maxima_data = {}\n",
    "distributions = ['gumbel_r', 'genextreme', 'pearson3']\n",
    "\n",
    "print(\"üìà ANALYSE FR√âQUENTIELLE DES MAXIMA ANNUELS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for station_name, df in station_data.items():\n",
    "    print(f\"\\nüåßÔ∏è Station : {station_name}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # Extraire maxima annuels\n",
    "    annual_max = df.groupby('year')['precipitation'].max()\n",
    "    maxima_data[station_name] = annual_max\n",
    "    \n",
    "    print(f\"üìä Statistiques maxima annuels:\")\n",
    "    print(f\"   Moyenne : {annual_max.mean():.1f} mm/24h\")\n",
    "    print(f\"   M√©diane : {annual_max.median():.1f} mm/24h\") \n",
    "    print(f\"   √âcart-type : {annual_max.std():.1f} mm/24h\")\n",
    "    print(f\"   Min/Max : {annual_max.min():.1f} / {annual_max.max():.1f} mm/24h\")\n",
    "    \n",
    "    # Test des distributions\n",
    "    print(f\"\\nüîç Test d'ajustement des distributions:\")\n",
    "    best_dist = None\n",
    "    best_aic = np.inf\n",
    "    \n",
    "    results = {}\n",
    "    for dist_name in distributions:\n",
    "        try:\n",
    "            dist = getattr(stats, dist_name)\n",
    "            params = dist.fit(annual_max)\n",
    "            \n",
    "            # Test Kolmogorov-Smirnov\n",
    "            ks_stat, ks_pval = stats.kstest(annual_max, \n",
    "                                          lambda x: dist.cdf(x, *params))\n",
    "            \n",
    "            # AIC (Akaike Information Criterion)\n",
    "            log_likelihood = np.sum(dist.logpdf(annual_max, *params))\n",
    "            aic = 2 * len(params) - 2 * log_likelihood\n",
    "            \n",
    "            results[dist_name] = {\n",
    "                'params': params,\n",
    "                'ks_stat': ks_stat,\n",
    "                'ks_pval': ks_pval,\n",
    "                'aic': aic\n",
    "            }\n",
    "            \n",
    "            print(f\"   {dist_name:12s}: KS p-value = {ks_pval:.3f}, AIC = {aic:.1f}\")\n",
    "            \n",
    "            if aic < best_aic:\n",
    "                best_aic = aic\n",
    "                best_dist = dist_name\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"   {dist_name:12s}: Erreur ajustement\")\n",
    "    \n",
    "    print(f\"   üèÜ Meilleure distribution: {best_dist}\")\n",
    "    \n",
    "    # Calculer quantiles avec meilleure distribution\n",
    "    if best_dist:\n",
    "        dist = getattr(stats, best_dist)\n",
    "        params = results[best_dist]['params']\n",
    "        \n",
    "        # P√©riodes de retour d'int√©r√™t\n",
    "        return_periods = [2, 5, 10, 20, 50, 100]\n",
    "        exceedance_probs = [1/T for T in return_periods]\n",
    "        quantiles = [dist.ppf(1-p, *params) for p in exceedance_probs]\n",
    "        \n",
    "        print(f\"\\nüíß Quantiles de pr√©cipitations ({best_dist}):\")\n",
    "        for T, q in zip(return_periods, quantiles):\n",
    "            print(f\"   T = {T:3d} ans : {q:6.1f} mm/24h\")\n",
    "        \n",
    "        # Stocker r√©sultats pour usage ult√©rieur\n",
    "        station_data[station_name]['best_dist'] = best_dist\n",
    "        station_data[station_name]['dist_params'] = params\n",
    "        station_data[station_name]['quantiles'] = dict(zip(return_periods, quantiles))\n",
    "\n",
    "print(f\"\\n‚úÖ Analyse fr√©quentielle termin√©e pour {len(maxima_data)} stations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06052c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualiser ajustements distributions pour quelques stations principales\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "main_stations = ['Abidjan_Aeroport', 'Abidjan_Ville', 'Grand_Bassam', 'Tiassale']\n",
    "\n",
    "for i, station_name in enumerate(main_stations):\n",
    "    ax = axes[i]\n",
    "    annual_max = maxima_data[station_name]\n",
    "    \n",
    "    # Histogramme donn√©es observ√©es\n",
    "    ax.hist(annual_max, bins=10, density=True, alpha=0.7, \n",
    "            color='lightblue', label='Observations')\n",
    "    \n",
    "    # Courbes distributions ajust√©es\n",
    "    x_range = np.linspace(annual_max.min() * 0.8, annual_max.max() * 1.2, 200)\n",
    "    \n",
    "    for dist_name in distributions:\n",
    "        if dist_name in ['gumbel_r', 'genextreme']:  # Afficher seulement principales\n",
    "            try:\n",
    "                dist = getattr(stats, dist_name)\n",
    "                # Utiliser param√®tres de la station si disponibles\n",
    "                if hasattr(station_data[station_name], 'get'):\n",
    "                    # R√©ajuster pour la visualisation\n",
    "                    params = dist.fit(annual_max)\n",
    "                else:\n",
    "                    params = dist.fit(annual_max)\n",
    "                \n",
    "                pdf_fitted = dist.pdf(x_range, *params)\n",
    "                ax.plot(x_range, pdf_fitted, linewidth=2, \n",
    "                       label=f'{dist_name.replace(\"_\", \" \").title()}')\n",
    "            except:\n",
    "                continue\n",
    "    \n",
    "    ax.set_xlabel('Pr√©cipitations max annuelles (mm/24h)')\n",
    "    ax.set_ylabel('Densit√© de probabilit√©')\n",
    "    ax.set_title(f'{station_name.replace(\"_\", \" \")}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576cd4ee",
   "metadata": {},
   "source": [
    "## üó∫Ô∏è √âtape 3 : Interpolation Spatiale (Krigeage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f113d1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©parer donn√©es pour interpolation spatiale\n",
    "# Utiliser quantiles T=20 ans comme exemple\n",
    "\n",
    "print(\"üó∫Ô∏è INTERPOLATION SPATIALE DES AL√âAS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Extraire coordonn√©es et quantiles T=20 ans\n",
    "stations_coords = []\n",
    "quantiles_20y = []\n",
    "\n",
    "for station_name, coords in stations_meteo.items():\n",
    "    stations_coords.append([coords['lon'], coords['lat']])\n",
    "    \n",
    "    # R√©cup√©rer quantile T=20 ans si disponible\n",
    "    if 'quantiles' in station_data[station_name]:\n",
    "        q20 = station_data[station_name]['quantiles'].get(20, 150)  # d√©faut 150mm\n",
    "    else:\n",
    "        # Estimer √† partir des maxima annuels\n",
    "        annual_max = maxima_data[station_name]\n",
    "        q20 = np.percentile(annual_max, 95)  # Approximation\n",
    "    \n",
    "    quantiles_20y.append(q20)\n",
    "\n",
    "stations_coords = np.array(stations_coords)\n",
    "quantiles_20y = np.array(quantiles_20y)\n",
    "\n",
    "print(f\"üìç {len(stations_coords)} stations pour interpolation\")\n",
    "print(f\"üíß Quantiles T=20 ans : {quantiles_20y.min():.1f} - {quantiles_20y.max():.1f} mm/24h\")\n",
    "\n",
    "# D√©finir grille d'interpolation haute r√©solution\n",
    "lon_min, lon_max = -4.8, -3.5\n",
    "lat_min, lat_max = 5.0, 6.2\n",
    "resolution = 0.02  # ~2km\n",
    "\n",
    "lon_grid = np.arange(lon_min, lon_max + resolution, resolution)\n",
    "lat_grid = np.arange(lat_min, lat_max + resolution, resolution)\n",
    "lon_mesh, lat_mesh = np.meshgrid(lon_grid, lat_grid)\n",
    "\n",
    "grid_points = np.column_stack([lon_mesh.ravel(), lat_mesh.ravel()])\n",
    "\n",
    "print(f\"üî≤ Grille interpolation : {len(lon_grid)}x{len(lat_grid)} = {len(grid_points)} points\")\n",
    "print(f\"üìè R√©solution : ~{resolution * 111:.1f} km\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47eee416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©thode 1: Interpolation simple (IDW - Inverse Distance Weighting)\n",
    "print(\"üîß M√©thode 1 : Interpolation IDW\")\n",
    "\n",
    "# Interpolation IDW\n",
    "quantiles_idw = griddata(stations_coords, quantiles_20y, grid_points, \n",
    "                        method='cubic', fill_value=np.nan)\n",
    "\n",
    "# Remplacer NaN par interpolation linear\n",
    "mask_nan = np.isnan(quantiles_idw)\n",
    "if mask_nan.any():\n",
    "    quantiles_idw[mask_nan] = griddata(stations_coords, quantiles_20y, \n",
    "                                      grid_points[mask_nan], method='linear')\n",
    "\n",
    "quantiles_idw = quantiles_idw.reshape(lon_mesh.shape)\n",
    "\n",
    "print(f\"‚úÖ Interpolation IDW termin√©e\")\n",
    "print(f\"üìä Valeurs interpol√©es : {np.nanmin(quantiles_idw):.1f} - {np.nanmax(quantiles_idw):.1f} mm/24h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bd583e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# M√©thode 2: Krigeage avec processus gaussien\n",
    "print(\"\\nüîß M√©thode 2 : Krigeage (Processus Gaussien)\")\n",
    "\n",
    "# Configurer kernel pour krigeage\n",
    "length_scale = 0.5  # √âchelle spatiale en degr√©s (~50km)\n",
    "noise_level = 5.0   # Niveau de bruit (mm/24h)\n",
    "\n",
    "kernel = RBF(length_scale=length_scale) + WhiteKernel(noise_level=noise_level)\n",
    "\n",
    "# Ajuster mod√®le\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=1e-10, \n",
    "                            n_restarts_optimizer=10, normalize_y=True)\n",
    "\n",
    "print(\"‚öôÔ∏è Ajustement du mod√®le de krigeage...\")\n",
    "gp.fit(stations_coords, quantiles_20y)\n",
    "\n",
    "print(f\"üìä Kernel optimis√© : {gp.kernel_}\")\n",
    "print(f\"üìä Log-vraisemblance : {gp.log_marginal_likelihood():.2f}\")\n",
    "\n",
    "# Validation crois√©e\n",
    "cv_scores = cross_val_score(gp, stations_coords, quantiles_20y, \n",
    "                           cv=min(5, len(stations_coords)), \n",
    "                           scoring='neg_mean_squared_error')\n",
    "rmse_cv = np.sqrt(-cv_scores.mean())\n",
    "\n",
    "print(f\"üìä RMSE validation crois√©e : {rmse_cv:.2f} mm/24h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede76555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©diction sur la grille compl√®te\n",
    "print(\"üéØ Pr√©diction spatiale...\")\n",
    "\n",
    "# Diviser en blocs pour √©viter probl√®mes m√©moire\n",
    "n_blocks = 4\n",
    "block_size = len(grid_points) // n_blocks\n",
    "\n",
    "quantiles_kriging = []\n",
    "uncertainties_kriging = []\n",
    "\n",
    "for i in range(n_blocks):\n",
    "    start_idx = i * block_size\n",
    "    end_idx = (i + 1) * block_size if i < n_blocks - 1 else len(grid_points)\n",
    "    \n",
    "    block_points = grid_points[start_idx:end_idx]\n",
    "    mean_pred, std_pred = gp.predict(block_points, return_std=True)\n",
    "    \n",
    "    quantiles_kriging.append(mean_pred)\n",
    "    uncertainties_kriging.append(std_pred)\n",
    "    \n",
    "    print(f\"   Bloc {i+1}/{n_blocks} trait√© ({len(block_points)} points)\")\n",
    "\n",
    "# Concat√©ner r√©sultats\n",
    "quantiles_kriging = np.concatenate(quantiles_kriging)\n",
    "uncertainties_kriging = np.concatenate(uncertainties_kriging)\n",
    "\n",
    "# Reshape en grilles\n",
    "quantiles_kriging = quantiles_kriging.reshape(lon_mesh.shape)\n",
    "uncertainties_kriging = uncertainties_kriging.reshape(lon_mesh.shape)\n",
    "\n",
    "print(f\"‚úÖ Krigeage termin√©\")\n",
    "print(f\"üìä Pr√©dictions : {quantiles_kriging.min():.1f} - {quantiles_kriging.max():.1f} mm/24h\") \n",
    "print(f\"üìä Incertitudes : {uncertainties_kriging.min():.1f} - {uncertainties_kriging.max():.1f} mm/24h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdbc4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparaison des m√©thodes d'interpolation\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "# 1. IDW\n",
    "im1 = axes[0,0].contourf(lon_mesh, lat_mesh, quantiles_idw, \n",
    "                        levels=15, cmap='Blues', extend='both')\n",
    "axes[0,0].scatter(stations_coords[:,0], stations_coords[:,1], \n",
    "                 c=quantiles_20y, s=100, cmap='Blues', \n",
    "                 edgecolors='black', linewidth=1)\n",
    "axes[0,0].set_title('Interpolation IDW')\n",
    "axes[0,0].set_xlabel('Longitude')\n",
    "axes[0,0].set_ylabel('Latitude')\n",
    "plt.colorbar(im1, ax=axes[0,0], label='Pr√©cipitations T=20 ans (mm/24h)')\n",
    "\n",
    "# 2. Krigeage\n",
    "im2 = axes[0,1].contourf(lon_mesh, lat_mesh, quantiles_kriging, \n",
    "                        levels=15, cmap='Blues', extend='both')\n",
    "axes[0,1].scatter(stations_coords[:,0], stations_coords[:,1], \n",
    "                 c=quantiles_20y, s=100, cmap='Blues', \n",
    "                 edgecolors='black', linewidth=1)\n",
    "axes[0,1].set_title('Krigeage (Processus Gaussien)')\n",
    "axes[0,1].set_xlabel('Longitude')\n",
    "axes[0,1].set_ylabel('Latitude')\n",
    "plt.colorbar(im2, ax=axes[0,1], label='Pr√©cipitations T=20 ans (mm/24h)')\n",
    "\n",
    "# 3. Incertitudes krigeage\n",
    "im3 = axes[1,0].contourf(lon_mesh, lat_mesh, uncertainties_kriging, \n",
    "                        levels=15, cmap='Reds', extend='both')\n",
    "axes[1,0].scatter(stations_coords[:,0], stations_coords[:,1], \n",
    "                 s=50, c='black', marker='x')\n",
    "axes[1,0].set_title('Incertitudes Krigeage')\n",
    "axes[1,0].set_xlabel('Longitude')\n",
    "axes[1,0].set_ylabel('Latitude')\n",
    "plt.colorbar(im3, ax=axes[1,0], label='√âcart-type (mm/24h)')\n",
    "\n",
    "# 4. Diff√©rence IDW - Krigeage\n",
    "diff = quantiles_idw - quantiles_kriging\n",
    "im4 = axes[1,1].contourf(lon_mesh, lat_mesh, diff, \n",
    "                        levels=15, cmap='RdBu_r', extend='both')\n",
    "axes[1,1].scatter(stations_coords[:,0], stations_coords[:,1], \n",
    "                 s=50, c='black', marker='x')\n",
    "axes[1,1].set_title('Diff√©rence IDW - Krigeage')\n",
    "axes[1,1].set_xlabel('Longitude')\n",
    "axes[1,1].set_ylabel('Latitude')\n",
    "plt.colorbar(im4, ax=axes[1,1], label='Diff√©rence (mm/24h)')\n",
    "\n",
    "# Ajouter noms des principales villes\n",
    "villes = {'Abidjan': [-4.024, 5.325], 'Grand-Bassam': [-3.738, 5.201], \n",
    "          'Dabou': [-4.377, 5.325], 'Tiassal√©': [-4.823, 5.898]}\n",
    "\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    for ville, coords in villes.items():\n",
    "        if coords[0] >= lon_min and coords[0] <= lon_max and coords[1] >= lat_min and coords[1] <= lat_max:\n",
    "            ax.plot(coords[0], coords[1], 'k*', markersize=8)\n",
    "            ax.annotate(ville, coords, xytext=(3, 3), textcoords='offset points', \n",
    "                       fontsize=8, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "455afd5b",
   "metadata": {},
   "source": [
    "## üéØ √âtape 4 : Cr√©ation de l'Objet Hazard CLIMADA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837a5ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er objet Hazard avec les r√©sultats du krigeage\n",
    "print(\"üèóÔ∏è CR√âATION OBJET HAZARD CLIMADA\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Utiliser r√©sultats krigeage (plus pr√©cis)\n",
    "centroids_lon_flat = lon_mesh.flatten()\n",
    "centroids_lat_flat = lat_mesh.flatten()\n",
    "intensities_flat = quantiles_kriging.flatten()\n",
    "\n",
    "# √âliminer points sur oc√©an (optionnel - garde tous pour simplicit√©)\n",
    "# land_mask = coord_on_land(centroids_lat_flat, centroids_lon_flat)\n",
    "land_mask = np.ones(len(centroids_lat_flat), dtype=bool)  # Garde tous\n",
    "\n",
    "# Filtrer\n",
    "centroids_lon_land = centroids_lon_flat[land_mask]\n",
    "centroids_lat_land = centroids_lat_flat[land_mask] \n",
    "intensities_land = intensities_flat[land_mask]\n",
    "\n",
    "print(f\"üó∫Ô∏è Points retenus : {len(centroids_lon_land)} / {len(centroids_lon_flat)}\")\n",
    "\n",
    "# Cr√©er √©v√©nements pour diff√©rentes p√©riodes de retour\n",
    "return_periods = [2, 5, 10, 20, 50, 100]\n",
    "n_events = len(return_periods)\n",
    "\n",
    "# Matrice d'intensit√©s (√©v√©nements x centroides)\n",
    "intensities_matrix = np.zeros((n_events, len(centroids_lon_land)))\n",
    "\n",
    "for i, T in enumerate(return_periods):\n",
    "    # Calculer facteur d'√©chelle par rapport √† T=20 ans\n",
    "    if T == 20:\n",
    "        scale_factor = 1.0\n",
    "    else:\n",
    "        # Approximation bas√©e sur distribution de Gumbel\n",
    "        # Relation empirique pour r√©gion tropicale\n",
    "        scale_factor = 1 + 0.15 * np.log(T / 20)\n",
    "    \n",
    "    intensities_matrix[i, :] = intensities_land * scale_factor\n",
    "\n",
    "print(f\"üìä Matrice intensit√©s : {n_events} √©v√©nements √ó {len(centroids_lon_land)} centroides\")\n",
    "print(f\"üíß Intensit√©s : {intensities_matrix.min():.1f} - {intensities_matrix.max():.1f} mm/24h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5263eb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©er objet Hazard final\n",
    "hazard = Hazard('FL')  # Flood hazard\n",
    "\n",
    "# Centroides\n",
    "hazard.centroids.set_lat_lon(centroids_lat_land, centroids_lon_land)\n",
    "\n",
    "# √âv√©nements\n",
    "hazard.event_id = np.arange(1, n_events + 1)\n",
    "hazard.event_name = [f'Precipitation_T{T}ans' for T in return_periods]\n",
    "hazard.date = pd.to_datetime([f'2023-06-{15+i}' for i in range(n_events)]).values\n",
    "hazard.frequency = 1.0 / np.array(return_periods)  # Fr√©quence annuelle\n",
    "hazard.orig = np.ones(n_events, dtype=bool)\n",
    "\n",
    "# Intensit√©s (matrice sparse pour efficacit√©)\n",
    "from scipy.sparse import csr_matrix\n",
    "hazard.intensity = csr_matrix(intensities_matrix)\n",
    "\n",
    "# M√©tadonn√©es\n",
    "hazard.units = 'mm/day'\n",
    "hazard.tag.description = 'Pr√©cipitations extr√™mes interpol√©es - R√©gion Abidjan - Formation DGE'\n",
    "hazard.tag.file_name = 'precipitations_abidjan_krigeage.hdf5'\n",
    "\n",
    "# Validation\n",
    "hazard.check()\n",
    "\n",
    "print(\"‚úÖ Objet Hazard cr√©√© et valid√©!\")\n",
    "print(f\"üìã R√©sum√© Hazard:\")\n",
    "print(f\"   - Type: {hazard.tag.haz_type}\")\n",
    "print(f\"   - √âv√©nements: {hazard.size[0]}\")\n",
    "print(f\"   - Centroides: {hazard.size[1]}\")\n",
    "print(f\"   - Fr√©quences: {hazard.frequency}\")\n",
    "print(f\"   - Intensit√©s moyennes par √©v√©nement:\")\n",
    "for i, (name, freq, T) in enumerate(zip(hazard.event_name, hazard.frequency, return_periods)):\n",
    "    mean_intensity = hazard.intensity[i, :].mean()\n",
    "    print(f\"     {name}: {mean_intensity:.1f} mm/24h (T={T} ans, f={freq:.3f}/an)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b720e673",
   "metadata": {},
   "source": [
    "## üìä √âtape 5 : Validation et Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57d9269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation 1: Coh√©rence spatiale\n",
    "print(\"üîç VALIDATION DE L'AL√âA CR√â√â\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# V√©rifier coh√©rence avec donn√©es stations\n",
    "print(\"üìç Validation aux points de stations:\")\n",
    "\n",
    "for station_name, coords in stations_meteo.items():\n",
    "    # Trouver centroide le plus proche\n",
    "    distances = np.sqrt((centroids_lon_land - coords['lon'])**2 + \n",
    "                       (centroids_lat_land - coords['lat'])**2)\n",
    "    nearest_idx = np.argmin(distances)\n",
    "    nearest_dist = distances[nearest_idx] * 111  # km approximatif\n",
    "    \n",
    "    # Comparer T=20 ans\n",
    "    observed_q20 = quantiles_20y[list(stations_meteo.keys()).index(station_name)]\n",
    "    modeled_q20 = hazard.intensity[3, nearest_idx]  # T=20 ans est index 3\n",
    "    \n",
    "    error = abs(modeled_q20 - observed_q20)\n",
    "    error_pct = (error / observed_q20) * 100\n",
    "    \n",
    "    print(f\"   {station_name:15s}: Obs={observed_q20:5.1f} mm, Mod={modeled_q20:5.1f} mm, \"\n",
    "          f\"Err={error_pct:4.1f}% (dist={nearest_dist:.1f}km)\")\n",
    "\n",
    "print(f\"\\nüìä Statistiques globales validation:\")\n",
    "all_errors = []\n",
    "for i, station_name in enumerate(stations_meteo.keys()):\n",
    "    coords = stations_meteo[station_name]\n",
    "    distances = np.sqrt((centroids_lon_land - coords['lon'])**2 + \n",
    "                       (centroids_lat_land - coords['lat'])**2)\n",
    "    nearest_idx = np.argmin(distances)\n",
    "    \n",
    "    observed = quantiles_20y[i]\n",
    "    modeled = hazard.intensity[3, nearest_idx]\n",
    "    error_pct = abs(modeled - observed) / observed * 100\n",
    "    all_errors.append(error_pct)\n",
    "\n",
    "print(f\"Erreur moyenne : {np.mean(all_errors):.1f}%\")\n",
    "print(f\"Erreur m√©diane : {np.median(all_errors):.1f}%\")\n",
    "print(f\"Erreur max : {np.max(all_errors):.1f}%\")\n",
    "print(f\"RMSE : {np.sqrt(np.mean(np.array(all_errors)**2)):.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d439c83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation 2: Analyse fr√©quentielle globale\n",
    "print(\"\\nüìà Validation fr√©quentielle:\")\n",
    "\n",
    "# Calculer statistiques par p√©riode de retour\n",
    "for i, T in enumerate(return_periods):\n",
    "    intensities_event = hazard.intensity[i, :].toarray().flatten()\n",
    "    \n",
    "    print(f\"T = {T:3d} ans: Moy={intensities_event.mean():5.1f} mm, \"\n",
    "          f\"Med={np.median(intensities_event):5.1f} mm, \"\n",
    "          f\"Min={intensities_event.min():5.1f} mm, \"\n",
    "          f\"Max={intensities_event.max():5.1f} mm\")\n",
    "\n",
    "# V√©rifier monotonie (T plus √©lev√© = intensit√© plus √©lev√©e)\n",
    "print(f\"\\nüéØ V√©rification monotonie:\")\n",
    "for i in range(1, len(return_periods)):\n",
    "    curr_mean = hazard.intensity[i, :].mean()\n",
    "    prev_mean = hazard.intensity[i-1, :].mean()\n",
    "    ratio = curr_mean / prev_mean\n",
    "    print(f\"T{return_periods[i]}/T{return_periods[i-1]} = {ratio:.3f} (attendu > 1.0)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b095753c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation finale de l'al√©a cr√©√©\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Afficher √©v√©nements pour diff√©rentes p√©riodes de retour\n",
    "selected_periods = [2, 10, 20, 50, 100]\n",
    "selected_indices = [return_periods.index(T) for T in selected_periods if T in return_periods]\n",
    "\n",
    "for i, (period_idx, T) in enumerate(zip(selected_indices, selected_periods)):\n",
    "    if i >= len(axes):\n",
    "        break\n",
    "        \n",
    "    ax = axes[i]\n",
    "    \n",
    "    # R√©cup√©rer intensit√©s pour cet √©v√©nement\n",
    "    intensities_event = hazard.intensity[period_idx, :].toarray().flatten()\n",
    "    \n",
    "    # Cr√©er scatter plot color√©\n",
    "    scatter = ax.scatter(centroids_lon_land, centroids_lat_land, \n",
    "                        c=intensities_event, s=20, cmap='Blues', alpha=0.8)\n",
    "    \n",
    "    # Ajouter stations m√©t√©o\n",
    "    ax.scatter(stations_coords[:,0], stations_coords[:,1], \n",
    "              c='red', s=80, marker='^', edgecolors='black', \n",
    "              linewidth=1, label='Stations m√©t√©o')\n",
    "    \n",
    "    # Ajouter contours\n",
    "    # Regridder pour contours\n",
    "    xi = np.linspace(centroids_lon_land.min(), centroids_lon_land.max(), 50)\n",
    "    yi = np.linspace(centroids_lat_land.min(), centroids_lat_land.max(), 50)\n",
    "    zi = griddata((centroids_lon_land, centroids_lat_land), intensities_event, \n",
    "                  np.meshgrid(xi, yi), method='cubic')\n",
    "    \n",
    "    contours = ax.contour(xi, yi, zi, levels=8, colors='black', alpha=0.4, linewidths=0.5)\n",
    "    ax.clabel(contours, inline=True, fontsize=8, fmt='%.0f')\n",
    "    \n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    ax.set_title(f'Pr√©cipitations T = {T} ans\\n(f = {hazard.frequency[period_idx]:.3f}/an)')\n",
    "    \n",
    "    # Colorbar\n",
    "    cbar = plt.colorbar(scatter, ax=ax)\n",
    "    cbar.set_label('mm/24h')\n",
    "    \n",
    "    if i == 0:\n",
    "        ax.legend()\n",
    "\n",
    "# Graphique 6: Courbe fr√©quence-intensit√© moyenne\n",
    "if len(axes) > len(selected_periods):\n",
    "    ax = axes[len(selected_periods)]\n",
    "    \n",
    "    mean_intensities = [hazard.intensity[i, :].mean() for i in range(n_events)]\n",
    "    max_intensities = [hazard.intensity[i, :].max() for i in range(n_events)]\n",
    "    min_intensities = [hazard.intensity[i, :].min() for i in range(n_events)]\n",
    "    \n",
    "    ax.semilogx(return_periods, mean_intensities, 'bo-', linewidth=2, \n",
    "                markersize=8, label='Moyenne')\n",
    "    ax.semilogx(return_periods, max_intensities, 'r^-', linewidth=1, \n",
    "                markersize=6, label='Maximum')\n",
    "    ax.semilogx(return_periods, min_intensities, 'gv-', linewidth=1, \n",
    "                markersize=6, label='Minimum')\n",
    "    \n",
    "    ax.set_xlabel('P√©riode de retour (ann√©es)')\n",
    "    ax.set_ylabel('Pr√©cipitations (mm/24h)')\n",
    "    ax.set_title('Courbe Intensit√©-Dur√©e-Fr√©quence\\nR√©gion Abidjan')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d34417d",
   "metadata": {},
   "source": [
    "## üíæ √âtape 6 : Sauvegarde et Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbf9cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder l'al√©a cr√©√©\n",
    "output_dir = Path('data/hazards')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Sauvegarder en format CLIMADA (HDF5)\n",
    "hazard_file = output_dir / 'precipitation_hazard_abidjan_formation.hdf5'\n",
    "hazard.write_hdf5(hazard_file)\n",
    "\n",
    "print(f\"üíæ Al√©a sauvegard√© : {hazard_file}\")\n",
    "print(f\"üìä Taille fichier : {hazard_file.stat().st_size / 1024 / 1024:.1f} MB\")\n",
    "\n",
    "# Sauvegarder m√©tadonn√©es en CSV\n",
    "metadata_df = pd.DataFrame({\n",
    "    'event_id': hazard.event_id,\n",
    "    'event_name': hazard.event_name,\n",
    "    'return_period': return_periods,\n",
    "    'frequency': hazard.frequency,\n",
    "    'date': hazard.date,\n",
    "    'mean_intensity': [hazard.intensity[i, :].mean() for i in range(n_events)],\n",
    "    'max_intensity': [hazard.intensity[i, :].max() for i in range(n_events)],\n",
    "    'min_intensity': [hazard.intensity[i, :].min() for i in range(n_events)]\n",
    "})\n",
    "\n",
    "metadata_file = output_dir / 'precipitation_hazard_metadata.csv'\n",
    "metadata_df.to_csv(metadata_file, index=False)\n",
    "\n",
    "print(f\"üìã M√©tadonn√©es sauvegard√©es : {metadata_file}\")\n",
    "\n",
    "# Sauvegarder centroides\n",
    "centroids_df = pd.DataFrame({\n",
    "    'centroid_id': range(len(centroids_lon_land)),\n",
    "    'longitude': centroids_lon_land,\n",
    "    'latitude': centroids_lat_land\n",
    "})\n",
    "\n",
    "centroids_file = output_dir / 'precipitation_hazard_centroids.csv'\n",
    "centroids_df.to_csv(centroids_file, index=False)\n",
    "\n",
    "print(f\"üìç Centroides sauvegard√©s : {centroids_file}\")\n",
    "\n",
    "# R√©sum√© technique pour documentation DGE\n",
    "technical_summary = f\"\"\"\n",
    "AL√âA PR√âCIPITATIONS - R√âGION ABIDJAN\n",
    "Formation DGE CLIMADA - {pd.Timestamp.now().strftime('%Y-%m-%d')}\n",
    "\n",
    "CARACT√âRISTIQUES TECHNIQUES:\n",
    "- Zone d'√©tude: {lon_min:.2f}¬∞/{lon_max:.2f}¬∞E, {lat_min:.2f}¬∞/{lat_max:.2f}¬∞N\n",
    "- R√©solution spatiale: {resolution:.3f}¬∞ (~{resolution * 111:.1f} km)\n",
    "- Nombre de centroides: {len(centroids_lon_land):,}\n",
    "- M√©thode d'interpolation: Krigeage (Processus Gaussien)\n",
    "- P√©riodes de retour: {', '.join(map(str, return_periods))} ans\n",
    "\n",
    "DONN√âES SOURCES:\n",
    "- {len(stations_meteo)} stations m√©t√©o SODEXAM (simul√©es)\n",
    "- P√©riode d'analyse: 1990-2023 ({n_years} ans)\n",
    "- Analyse fr√©quentielle: distributions de Gumbel/GEV\n",
    "\n",
    "VALIDATION:\n",
    "- Erreur moyenne aux stations: {np.mean(all_errors):.1f}%\n",
    "- RMSE validation crois√©e: {rmse_cv:.2f} mm/24h\n",
    "- Coh√©rence spatiale: v√©rifi√©e\n",
    "- Monotonie fr√©quentielle: v√©rifi√©e\n",
    "\n",
    "FICHIERS G√âN√âR√âS:\n",
    "- {hazard_file.name}: Objet Hazard CLIMADA\n",
    "- {metadata_file.name}: M√©tadonn√©es √©v√©nements\n",
    "- {centroids_file.name}: Coordonn√©es centroides\n",
    "\"\"\"\n",
    "\n",
    "doc_file = output_dir / 'DOCUMENTATION_ALEA_PRECIPITATION.txt'\n",
    "with open(doc_file, 'w', encoding='utf-8') as f:\n",
    "    f.write(technical_summary)\n",
    "\n",
    "print(f\"üìÑ Documentation technique : {doc_file}\")\n",
    "print(f\"\\n‚úÖ Sauvegarde compl√®te termin√©e!\")\n",
    "print(f\"üìÅ Dossier de sortie : {output_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47929897",
   "metadata": {},
   "source": [
    "## üéØ Exercices Pratiques\n",
    "\n",
    "### üéØ Exercice 1: Modification de la r√©solution spatiale\n",
    "Recr√©ez l'al√©a avec une r√©solution diff√©rente (0.01¬∞ ou 0.05¬∞) et comparez les r√©sultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8277ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCICE 1: Modifier la r√©solution et analyser l'impact\n",
    "# Testez resolution = 0.01 (plus fin) ou 0.05 (plus grossier)\n",
    "\n",
    "# Votre code ici:\n",
    "# ---------------\n",
    "\n",
    "\n",
    "\n",
    "# ---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6036b88d",
   "metadata": {},
   "source": [
    "### üéØ Exercice 2: Ajout de nouvelles stations\n",
    "Ajoutez 3 stations fictives et observez l'impact sur l'interpolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d762234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCICE 2: Ajouter stations m√©t√©o suppl√©mentaires\n",
    "# Indice: Modifiez le dictionnaire stations_meteo\n",
    "\n",
    "# Votre code ici:\n",
    "# ---------------\n",
    "\n",
    "\n",
    "\n",
    "# ---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299e56e1",
   "metadata": {},
   "source": [
    "### üéØ Exercice 3: Comparaison de kernels de krigeage\n",
    "Testez diff√©rents kernels (Mat√©rn, RationalQuadratic) et comparez les performances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3875e458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCICE 3: Tester diff√©rents kernels pour le krigeage\n",
    "# Imports additionnels: Matern, RationalQuadratic\n",
    "# from sklearn.gaussian_process.kernels import Matern, RationalQuadratic\n",
    "\n",
    "# Votre code ici:\n",
    "# ---------------\n",
    "\n",
    "\n",
    "\n",
    "# ---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "890c82c5",
   "metadata": {},
   "source": [
    "## üéØ R√©sum√© et Points Cl√©s\n",
    "\n",
    "### ‚úÖ Comp√©tences acquises:\n",
    "\n",
    "1. **Analyse fr√©quentielle avanc√©e**:\n",
    "   - Extraction maxima annuels\n",
    "   - Ajustement distributions (Gumbel, GEV, Pearson III)\n",
    "   - Calcul quantiles et p√©riodes de retour\n",
    "   - Tests de validation statistique\n",
    "\n",
    "2. **Interpolation spatiale**:\n",
    "   - M√©thodes IDW et krigeage\n",
    "   - Optimisation de kernels\n",
    "   - Quantification des incertitudes\n",
    "   - Validation crois√©e\n",
    "\n",
    "3. **Cr√©ation d'al√©as CLIMADA**:\n",
    "   - Objets Hazard haute r√©solution\n",
    "   - Matrices d'intensit√© sparse\n",
    "   - M√©tadonn√©es compl√®tes\n",
    "   - Sauvegarde formats multiples\n",
    "\n",
    "4. **Applications pratiques**:\n",
    "   - Donn√©es SODEXAM r√©alistes\n",
    "   - Validation terrain\n",
    "   - Documentation technique\n",
    "   - Export pour utilisateurs DGE\n",
    "\n",
    "### üîÑ Prochaines √©tapes:\n",
    "- TP3: Mod√©lisation exposition √©conomique d√©taill√©e\n",
    "- TP4: Fonctions de dommage sectorielles\n",
    "- TP5: Calcul impacts et analyse co√ªt-b√©n√©fice\n",
    "\n",
    "### üìû Support:\n",
    "**formation.climada.dge@gouv.ci**\n",
    "\n",
    "---\n",
    "**üìÖ Formation DGE - CLIMADA C√¥te d'Ivoire 2025**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
